{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71030dcc",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2f24ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from time import strftime\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b60bd",
   "metadata": {},
   "source": [
    "## 1. `basic_clean` function\n",
    "This function should take in a string and apply some basic text cleaning to it:\n",
    "1. Lowercase everything,\n",
    "2. Normalize unicode characters, and\n",
    "3. Replace anything that is not a letter, number, whitespace or a single quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b6e607c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncarrot cake brownie carrot cake ice cream croissant powder bear claw icing tiramisu souffle fruitcake carrot \\ncake macaroon liquorice dragee sweet icing lollipop chocolate bar jelly beans\\ncake sugar plum cookie tiramisu dessert cupcake sweet lollipop liquorice dragee ice cream pastry shortbread \\nhalvah chupa chups sweet ice cream cheesecake pastry powder donut cake\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. lowercase\n",
    "string = '''\n",
    "Carrot cake brownie carrot cake ice cream croissant powder bear claw. Icing tiramisu soufflé fruitcake carrot \n",
    "cake macaroon liquorice. Dragée sweet icing lollipop chocolate bar jelly beans.\n",
    "Cake sugar plum cookie tiramisu dessert cupcake sweet lollipop liquorice. Dragée ice cream pastry shortbread \n",
    "halvah chupa chups sweet ice cream. Cheesecake pastry powder donut cake.\n",
    "'''.lower()#.strip().lower()\n",
    "\n",
    "# 2. normalize unicode\n",
    "string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "# 3. replace non-alphanumeric characters\n",
    "string = re.sub(r\"[^a-z0-9'\\s]\", '', string)#.replace('\\n', ' ')\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd1553c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string as a paramenter and performs the following basic cleaning functions:\n",
    "        1. lowercase,\n",
    "        2. normalize unicode, and\n",
    "        3. remove non-alphanumeric characters\n",
    "    \n",
    "    The function returns the cleaned string.\n",
    "    '''\n",
    "    # 1. lowercase\n",
    "    string = string.strip().lower()\n",
    "    \n",
    "    # 2. normalize unicode\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # 3. replace non-alphanumeric characters\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c51f1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"9po6ykrq \\x0cv3tu\\n7ig'kots64saio'k\\njmnrilyetqams63f\\nh821e83qyw69emer\\nrsvbl72qzfmv33bmu\\n7p 7ditjdzjpr9sicm\\nqvadpsz7vuwglvx\\nn1trtuweit5pq8j0q7o\\nrls9dozscruop8m\\nnz8lrcfvksubhd2d'\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tesitng the function\n",
    "basic_clean(\"\"\"\n",
    "-9p]o.6ykrq` \\14v3tu\n",
    "7ig'kots64s[ai.o'[k/\n",
    "jmnr/;ilyetqams6/;3f\n",
    "h821\\e83qyw69eme.;r,\n",
    "rsvb,l72qzfmv33bm,u=\n",
    "7p 7ditj\\dzjpr9s=icm\n",
    "qva,d],p;sz7vu[wglvx\n",
    "n1trtu;weit5pq8j0q7o\n",
    "rls9dozsc`ruop8m.-\\0\n",
    "nz8lr=cfvksu/bhd-2d'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccacc5",
   "metadata": {},
   "source": [
    "## 2. `tokenize` function\n",
    "This function should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2ab3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"\n",
    "-9p]o.6ykrq` \\14v3tu\n",
    "7ig'kots64s[ai.o'[k/\n",
    "jmnr/;ilyetqams6/;3f\n",
    "h821\\e83qyw69eme.;r,\n",
    "rsvb,l72qzfmv33bm,u=\n",
    "7p 7ditj\\dzjpr9s=icm\n",
    "qva,d],p;sz7vu[wglvx\n",
    "n1trtu;weit5pq8j0q7o\n",
    "rls9dozsc`ruop8m.-\\0\n",
    "nz8lr=cfvksu/bhd-2d'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7f8e8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-9p ] o.6ykrq ` \\x0cv3tu\\n7ig ' kots64s[ ai.o ' [ k/\\njmnr/ ; ilyetqams6/ ; 3f\\nh821\\\\e83qyw69eme. ; r , \\nrsvb , l72qzfmv33bm , u=\\n7p 7ditj\\\\dzjpr9s=icm\\nqva , d ] , p ; sz7vu[ wglvx\\nn1trtu ; weit5pq8j0q7o\\nrls9dozsc ` ruop8m.-\\x00\\nnz8lr=cfvksu/bhd-2d '\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "tokenizer.tokenize(test_string, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5d47f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and returns the tokenized version of that string.\n",
    "    '''\n",
    "    \n",
    "    # creating tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # using tokenizer object on string\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d1a91d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a string. Let ' s see what happens when we tokenize it !\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('''\n",
    "This is a string. Let's see what happens when we tokenize it!\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39150046",
   "metadata": {},
   "source": [
    "## 3. `stem` function\n",
    "This function should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4086e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string, stems each individual word, and then joins\n",
    "    the stem words back together in the returned string.\n",
    "    '''\n",
    "    \n",
    "    # creating the stem object\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # creating the stems for each individual word in the string\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # putting the stemmed words back together into string\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd538ce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carrot cake browni carrot cake ice cream croissant powder bear claw ice tiramisu souffl fruitcak carrot cake macaroon liquoric drage sweet ice lollipop chocol bar jelli bean cake sugar plum cooki tiramisu dessert cupcak sweet lollipop liquoric drage ice cream pastri shortbread halvah chupa chup sweet ice cream cheesecak pastri powder donut cake'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c759c7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cake      5\n",
       "carrot    3\n",
       "sweet     3\n",
       "ice       3\n",
       "cream     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(string.split()).value_counts().head(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0dbbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6062595",
   "metadata": {},
   "source": [
    "## 4. `lemmatize` function\n",
    "This function should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a9afb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # create the lemmatization object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # creating a list of string of each word in the article and applying the lemmatize object to each word\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # joining the individual list of lemma string words to a single string of words\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0e22abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carrot cake brownie carrot cake ice cream croissant powder bear claw icing tiramisu souffle fruitcake carrot cake macaroon liquorice dragee sweet icing lollipop chocolate bar jelly bean cake sugar plum cookie tiramisu dessert cupcake sweet lollipop liquorice dragee ice cream pastry shortbread halvah chupa chups sweet ice cream cheesecake pastry powder donut cake'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6317e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncarrot cake brownie carrot cake ice cream croissant powder bear claw icing tiramisu souffle fruitcake carrot \\ncake macaroon liquorice dragee sweet icing lollipop chocolate bar jelly beans\\ncake sugar plum cookie tiramisu dessert cupcake sweet lollipop liquorice dragee ice cream pastry shortbread \\nhalvah chupa chups sweet ice cream cheesecake pastry powder donut cake\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233de91a",
   "metadata": {},
   "source": [
    "## 5. `remove_stopwords`\n",
    "This function should accept some text and return the text after removing all the stopwords.\n",
    "<br>\n",
    "It should also define two parameters:\n",
    "- `extra-words` any additional stop words (words we want removed that are not already listed as stop words)\n",
    "- `exclude-words` any words you want excluded from stop words search (stop words we don't want removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ff192e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls a list of common english stopwords\n",
    "stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d03f940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary', 'little', 'lamb.', 'Little', 'lamb.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the list of english stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "# savings the words in the string as a list of strings for each indidvidual word\n",
    "words = 'Mary had a little lamb. Little lamb.'.split()\n",
    "\n",
    "# creating a for loop that will loop through each of the individual words in the string\n",
    "#     and return a list of only the words that are not in the list of stopwords\n",
    "[word for word in words \\\n",
    "    if word not in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ee422f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    '''\n",
    "    This function takes in a string of words and splits it into a list of strings for each individual \n",
    "    word. It then loops through the list of words and returns a string of the joined string words, \n",
    "    excluding the stopwords.\n",
    "    '''\n",
    "    \n",
    "    # splitting the string of words into a list of strings\n",
    "    words = string.split()\n",
    "    \n",
    "    # saving the stop words\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # looping through the list of words and creating a new list with the words not in the stopwords list\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # joining the list of words back to a string of words\n",
    "    filtered_string = ' '.join(filtered_words)\n",
    "    \n",
    "    # printing number of words removed\n",
    "    print(f'Removed {len(string) - len(filtered_string)} stop words.\\n> Original string: {len(string)}\\n> New string: {len(filtered_string)}')\n",
    "    print()\n",
    "    \n",
    "    return filtered_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af4d2048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 18 stop words.\n",
      "> Original string: 95\n",
      "> New string: 77\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This little light mine, I\"m going let shine. Let shine, let shine, let shine.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing function\n",
    "remove_stopwords('This little light of mine, I\"m going to let it shine. Let it shine, let it shine, let it shine.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fd81b",
   "metadata": {},
   "source": [
    "## 6. `news_df`\n",
    "Use your data from the acquire to produce a dataframe of the news articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef519b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42ac1227",
   "metadata": {},
   "source": [
    "## 7. `codeup_df`\n",
    "Make another dataframe from the codeup blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796edca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0e8d3e",
   "metadata": {},
   "source": [
    "## 8. DataFrame columns\n",
    "For each dataframe, produce the following columns:\n",
    "- `title` to hold the title\n",
    "- `original` to hold the original article/post content\n",
    "- `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "- `stemmed` to hold the stemmed version of the cleaned data.\n",
    "- `lemmatized` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0dd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41f6de6d",
   "metadata": {},
   "source": [
    "## 9. Ask youself:\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d580b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
